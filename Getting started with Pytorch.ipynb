{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-36 *\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  5.6886  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "  5.7186  0.0000  0.0000\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a randomly initialized matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6540  0.3439  0.7411\n",
      " 0.3482  0.0477  0.3684\n",
      " 0.4323  0.5606  0.6292\n",
      " 0.5270  0.3122  0.9339\n",
      " 0.9407  0.8461  0.6374\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "#get size\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.0177  0.8002  1.7019\n",
      " 0.5623  0.8811  0.8897\n",
      " 0.5863  0.8955  0.7202\n",
      " 0.7640  0.5124  1.0128\n",
      " 1.9275  1.3658  0.8027\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#addition\n",
    "\n",
    "#syntax 1\n",
    "y = torch.rand(5,3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.0177  0.8002  1.7019\n",
      " 0.5623  0.8811  0.8897\n",
      " 0.5863  0.8955  0.7202\n",
      " 0.7640  0.5124  1.0128\n",
      " 1.9275  1.3658  0.8027\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#syntax 2\n",
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.0177  0.8002  1.7019\n",
      " 0.5623  0.8811  0.8897\n",
      " 0.5863  0.8955  0.7202\n",
      " 0.7640  0.5124  1.0128\n",
      " 1.9275  1.3658  0.8027\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#providing tensor as argument\n",
    "result = torch.Tensor(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.0177  0.8002  1.7019\n",
      " 0.5623  0.8811  0.8897\n",
      " 0.5863  0.8955  0.7202\n",
      " 0.7640  0.5124  1.0128\n",
      " 1.9275  1.3658  0.8027\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in place\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.3439\n",
      " 0.0477\n",
      " 0.5606\n",
      " 0.3122\n",
      " 0.8461\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resizing: if you want to resize/reshape tensor, you can use `torch.view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8) # -1 is inferred from other dimensions (8)\n",
    "a = x.view(-1,2)\n",
    "\n",
    "print(x.size(), y.size(), z.size(), a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert torch tensor to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numpy to tensor\n",
    "\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b =torch.from_numpy(a)\n",
    "np.add(a,1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafid\\Dropbox\\3.SelfStudy\\ml_lab\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\tensor.py:293: UserWarning: self and other not broadcastable, but have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.add(other)\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of a computation graph is essential to efficient deep learning programming, because it allows you to not have to write back propagation gradients yourself.\n",
    "\n",
    "A computation graph is simply a specification of how your data is combined to give you the output.\n",
    "\n",
    "Since the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives. This probably sounds vague, so lets see what is going on using the fundamental class of Pytorch: autograd.Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think from a programmers perspective. What is stored in the torch.Tensor objects we were creating above? Obviously the data and the shape, and maybe a few other things. But when we added two tensors together, we got an output tensor. All this output tensor knows is its data and shape. It has no idea that it was the sum of two other tensors (it could have been read in from a file, it could be the result of some other operation, etc...)\n",
    "\n",
    "The Variable class keeps track of how it was created. Lets see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "<AddBackward1 object at 0x000001A207396EF0>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# variables wrap tensor objects\n",
    "x = autograd.Variable(torch.Tensor([1.,2.,3.]), requires_grad=True)\n",
    "\n",
    "# you can access the data with the .data attribute\n",
    "print(x.data)\n",
    "\n",
    "# you can also do all the same operations you did with tensors with Variables\n",
    "y = autograd.Variable(torch.Tensor([4.,5.,6.]), requires_grad=True)\n",
    "z = x+y\n",
    "print(z.data)\n",
    "\n",
    "# BUT z knows something extra\n",
    "print(z.grad_fn)\n",
    "print(x.grad_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Variables know what created them. z knows that it wasn't read in from a file, it wasn't the result of a multiplication or exponential operation or whatever. And if you keep following z.grad_fn, you will find yourself at x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4\n",
      " 5\n",
      " 6\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that help us compute gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "<SumBackward0 object at 0x000001A2073FE0B8>\n"
     ]
    }
   ],
   "source": [
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.backward() # calling .backward() on any variable will run backprop, starting from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x000001A20741ADA0>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 2))\n",
    "y = torch.randn((2, 2))\n",
    "z = x + y  # These are Tensor types, and backprop would not be possible\n",
    "\n",
    "var_x = autograd.Variable(x, requires_grad=True)\n",
    "var_y = autograd.Variable(y, requires_grad=True)\n",
    "# var_z contains enough information to compute gradients, as we saw above\n",
    "var_z = var_x + var_y\n",
    "print(var_z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x000001A207429320>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 2))\n",
    "y = torch.randn((2, 2))\n",
    "z = x + y  # These are Tensor types, and backprop would not be possible\n",
    "\n",
    "var_x = autograd.Variable(x, requires_grad=True)\n",
    "var_y = autograd.Variable(y, requires_grad=True)\n",
    "# var_z contains enough information to compute gradients, as we saw above\n",
    "var_z = var_x + var_y\n",
    "print(var_z.grad_fn)\n",
    "\n",
    "var_z_data = var_z.data  # Get the wrapped Tensor object out of var_z...\n",
    "# Re-wrap the tensor in a new variable\n",
    "new_var_z = autograd.Variable(var_z_data)\n",
    "\n",
    "# ... does new_var_z have information to backprop to x and y?\n",
    "# NO!\n",
    "print(new_var_z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we see that at first var_z contains the gradient information. In new_var_z however, this information is no longer there, since we've redefined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-1.2231 -1.5287\n",
       "-0.4699  0.2091\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_z_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-1.2231 -1.5287\n",
       "-0.4699  0.2091\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_z.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x000001A20741AAC8>\n"
     ]
    }
   ],
   "source": [
    "print(var_z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.2231 -1.5287\n",
      "-0.4699  0.2091\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(new_var_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(new_var_z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
